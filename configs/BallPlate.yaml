ppo:
  num_timesteps: 100_000_000
  num_evals: 100
  reward_scaling: 10
  episode_length: 1000
  normalize_observations: True
  action_repeat: 1
  unroll_length: 5
  num_minibatches: 32
  num_updates_per_batch: 4
  discounting: 0.995
  learning_rate: 0.0001
  entropy_cost: 0.01
  num_envs: 1024
  batch_size: 1024
  seed: 1

experiment:
  regularizers:
    - 0
    - 1
    - 5
    - 10
    - 15
    - 20
    - 25
  noise:
    - 0
    - 0.05
    - 0.1
    - 0.15
    - 0.2
    - 0.25
    - 0.3
    - 0.35
    - 0.4
    - 0.45
    - 0.5
    - 0.55
    - 0.6
    - 0.65
    - 0.7
    - 0.75
    - 0.8
    - 0.85
    - 0.9
    - 0.95
    - 1