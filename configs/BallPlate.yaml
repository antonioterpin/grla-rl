ppo:
  num_timesteps: 100_000_000
  num_evals: 100
  reward_scaling: 10
  episode_length: 1000
  normalize_observations: True
  action_repeat: 1
  unroll_length: 5
  num_minibatches: 32
  num_updates_per_batch: 4
  discounting: 0.995
  learning_rate: 0.0001
  entropy_cost: 0.01
  num_envs: 1024
  batch_size: 1024
  seed: 1

experiment:
  regularizers:
    - 0
    - 1
    - 5
    - 10
    - 15
    - 20
    - 25
    - 30
    - 35
    - 40
    - 45
    - 50
  noise:
    - 0
    - 0.05
    - 0.1
    - 0.15
    - 0.2
    - 0.25
    - 0.3
    - 0.35
    - 0.4
    - 0.45
    - 0.5
    - 0.55
    - 0.6
    - 0.65
    - 0.7
    - 0.75
    - 0.8
    - 0.85
    - 0.9
    - 0.95
    - 1
    - 1.05
    - 1.1
    - 1.15
    - 1.2
    - 1.25
    - 1.3
    - 1.35
    - 1.4
    - 1.45
    - 1.5
    - 1.55
    - 1.6
    - 1.65
    - 1.7
    - 1.75
    - 1.8
    - 1.85
    - 1.9
    - 1.95
    - 2
    - 2.05
    - 2.1
    - 2.15
    - 2.2
    - 2.25
    - 2.3
    - 2.35
    - 2.4
    - 2.45
    - 2.5
    - 2.55
    - 2.6
    - 2.65
    - 2.7
    - 2.75
    - 2.8
    - 2.85
    - 2.9
    - 2.95
    - 3